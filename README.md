# DocuLlama-AI
# ğŸ“„ DocuLlama-AI â€” Chat with Your PDFs Using LLaMA Locally

DocuLlama-AI lets you **upload a PDF and interact with it conversationally**, powered entirely by a **locally-running LLaMA model** via **Ollama** and **LangChain**.

> ğŸ” 100% local. No API keys. No cloud. Just you and your documents.

---

## ğŸš€ Features

- ğŸ“ Upload any PDF (notes, books, contracts, etc.)
- ğŸ§  Ask any question about its content
- ğŸ’¬ Instant LLM-powered replies using **LLaMA3**
- ğŸ” Context-aware retrieval using **vector embeddings**
- ğŸ”Œ Works completely offline (once set up)

---

## ğŸ§  How It Works

1. PDF is converted into chunks of text  
2. Chunks are embedded into vectors using `mxbai-embed-large`  
3. Stored in a **Chroma** vector database  
4. A retriever finds the most relevant chunks  
5. A **locally-running LLaMA 3** model answers your query using LangChain + RAG

---

## ğŸ› ï¸ Setup Instructions
pip install -r requirements.txt

### âœ… 1. Install Ollama

Install Ollama (to run LLaMA 3 locally):

ollama pull llama3
ollama pull mxbai-embed-large




